merged paths from gc roots calculation may slow down the leak suspects report. null this problem was reported in the mat dev mailing list from  daniel le berre. i add to the bugzilla message his first mail and my first reply, which should explain the problem in detail.   -----original message----- from: mat-dev-bounces&#64;eclipse.org [mailto:mat-dev-bounces&#64;eclipse.org] on behalf of daniel le berre sent: sonntag, 21. juni 2009 23:12 to: mat-dev&#64;eclipse.org subject: [mat-dev] test case for leak detection algorithm  dear mat developers,  i gave a try to mat on sat4j under heavy duty  memory analyzer (incubation)    .  i generated a dump as suggested in ian bull blog: <a href="http://eclipsesource.com/blogs/2009/06/20/eclipse-galileo-feature-top-10">http://eclipsesource.com/blogs/2009/06/20/eclipse-galileo-feature-top-10</a> -list-number-5/  the dump is available here: <a href="http://www.sat4j.org/heap.bin.gz">http://www.sat4j.org/heap.bin.gz</a>  if i try to detect memory leaks on that dump, the leak detection algorithm cannot complete its task in reasonable time (it ran during at least 15mn on my piv 3ghz). i have to quit eclipse to stop the process (pushing the cancel button has no effect).  the dump is unusual i guess because there is a very high number of one single kind of objects (the constraints).  i am wondering if there is a hope to be able to use mat on such kind of dumps at some point or if those cases cannot be analyzed efficiently under current knowledge (i.e. limitation due to the complexity of the algorithms used).  cheers,      daniel     -----original message----- from: mat-dev-bounces&#64;eclipse.org [mailto:mat-dev-bounces&#64;eclipse.org] on behalf of tsvetkov, krum sent: montag, 22. juni 2009 15:32 to: memory analyzer dev list subject: re: [mat-dev] test case for leak detection algorithm  hello daniel,  i have some better explanation for the problem, however, no really good solution for the moment.  in your heapdump, the leak suspect is not a single object, but a group of over  objects from the same class. once we find the suspect, we attempt to provide in the report some information about the paths to the suspect. as there are many objects in the suspect, what the tool does is calculate only the shortest path from the gc roots to each of them and then try to merge the paths in order to show the common part. this could be a rather costly operation, therefore the tool takes randomly only  out of the  objects, and calculates the paths to them. this usually works pretty well. we also try to cache the already found paths and reuse them during the computation.  however, in your heap dump there are over hundred thousand different reference chains to every of the objects of interest, which happen to be processed before we find the cached path. and thus, the calculation of each path takes between  and  seconds on a relatively good pc. doing this for  objects will take time.  the optimal solution (which i don't have :-) ... yet ...) will be to find a better algorithm for the heavy computation i am doing. this may need some time, and at the end i'm not sure i could make it at all. at least i have a very good example now.  a not so optimal solution, but something you can use as a workaround for now, will be to use the &quot;find leaks&quot; query instead of running the &quot;leak suspects&quot; report - see attachment. there you can specify the maximum number of paths to be calculated (the parameter &quot;-max_paths&quot;). reducing the value there can help get a result for your heap dump faster.  thanks again for giving the feedback and the sample dump. i hope i could give you a reasonable workaround for now. and i also hope we'll be able to find some reasonable general solution for this.  regards, krum i have reimplemented the way the paths to multiple objects are calculated (revision #372 in svn). for the provided heap dump which was the trigger for this bugzilla entry, now the report is generated in seconds (instead of running over 20 min).  the previous implementation was doing a breadth first search (bfs) from every single object of interest back (using the inbound references) to the first encountered gc root.  now i implemented it in such a way, that only one bfs is done. it starts from the gc roots, and using the outbound refereces continues until all objects of interest are reached.  unfortunatelly i can't say that the fix has only positive effects. in some cases it may take longer than before to calculate the paths. namely, if you search the paths to just a few objects in a huge heap dump, it may take longer to reach them starting from the gc roots. in the worst cases almost the whole graph has to be traversed. but still the approach is much more predictable than the previous implementation.   i tried to summarize the pros and cons below, and i hope to get your comments on it:  ---------- pros: ---------- - on smaller heap dumps (about 1gb) the new algorithm is in general faster: for large number of objects the paths are found much faster, for a small amount of objects it may be slower, but the slownes is less than a second  - both on small (about 1gb) and large dumps (over 3gb) the new algorithm is much more predictable. it will take at most the time to traverse the graph once, which should be even on the huge dumps within a minute. with the old algorithm there were scenarios that took unpredictibly long (half an hour), or ran into an outofmemoryerror  - it will allow us to inclrease the limit for number of paths in the leak report. right now it is 10 000, and this was already too much with the provided sample heap dump. with the new algorithm the paths to all  objects are calculated within seconds  - the new algorithm gives me the possibility to better show the maximum remaining time in a progress bar  - the information which is gathered during the bfs can be persisted and reused for faster calculation of paths to other objects. i haven't done this, but it is possible.  - shorter and (hopefully) clearer code  --------- cons: --------- - on some large heap dumps (over 3gb), if one calculates the paths to a few objects only, then the new algorithm may be slower. in some of the tests i did, the old one took like 8 sec and the new one 40 sec. still, the time to finish the whole calculation is predictable, and if the action is done manual the progress bar shows adequate maximum time. ---------  i will be happy if some of the people on the list can comment.  i have already submitted the changes and if you take the latest nightly build you can try them out.  i thought about keeping the old and new algorithm and continue to use the old one for a small amount of objects, but then it is difficult to define this &quot;small amount&quot;. with the provided sample dump the calculation for 100 objects were already slow enough. additionally it will be harder to maintain the both pieces of code. therefore i trew the old one for the moment away.  looking forward to your replies. cool! feedback so far was positive. also our own experience was better than with the previous implementation. i think we can close this issue. (out of memory error ) 281311 281311 281311 281311
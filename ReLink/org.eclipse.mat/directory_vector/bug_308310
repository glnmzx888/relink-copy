isnapshot api doesn't handle very large arrays. null it is possible for some array objects to be bigger than integer.max_value. e.g. long big[integer.max_value]; could be approximately 0x400000008l bytes in length.  int isnapshot.getheapsize()  limits the size to integer.max_value, and the array size index file stores ints.  we need to consider how to deal with these very large objects, preferably maintaining some compatibility with previous versions. changing isnapshot.getheapsize to return a long also means changing long iobject.getusedheapsize() long abstractobjectimpl.getusedheapsize() long classimpl.getusedheapsize() classimpl.addinstance(long usedheapsize) classimpl.removeinstance(long usedheapsize) long instanceimpl.getusedheapsize() long objectarray.getusedheapsize() long primitivearray.getusedheapsize()  should classimpl.setheapsizeperinstance() and classimpl.getheapsizeperinstance()  be changed? these are only valid for non-array objects, so won't ever overflow an int.  these changes then cause some minor changes elsewhere:  e.g. arraybysizequery threadinfoimpl dominatorquery multiplepaths2gcrootsquery path2gcrootsquery garbagecleaner snapshotimpl changing the return type of a method is not a binary compatible change, so any user plug-ins using getusedheapsize would need to be recompiled.  is this acceptable? should we add a getshallowheapsize (or similar name, suggestions welcome) and deprecate getusedheapsize? should we ignore just the problem and limit the sizes to integer.max_value? i think that as there are real cases where using int is a limitation, we need to change, and we shouldn't ignore the problem. i though what would be the most acceptable way to change, and i think we should follow your suggestion to deprecate the old method and offer a new one:  long getshallowheapsize(int objectid) throws snapshotexception;  this way we won't break external queries that used this method. and we will have the time to adapt mat own queries to use the new one.  one thing which i am curious about - what would be the impact on performance / memory consumption? i suggest that we do some measurements before/after of building histograms - this is a very common operation, part of most queries. the parser interface needs to be changed slightly to accumulate sizes as longs.  ione2sizeindex extends ione2oneindex but adds a long getsize(int index) method.  sizeindexcollectoruncompressed instead of intindexcollectoruncompressed allows array sizes of longs to be accumulated in an int array.  in the writeto method a sizeindexreader then wraps the intindexreader returned from the intindexstreamer.  this sizeindexreader can uncompress a size using the getsize() method.  the indexmanager needs to use the sizeindexreader for array sizes.  uses of array2size.get() need to be changed to getsize() i've changed  classimpl.setheapsizeperinstance() classimpl.setusedheapsize() and iclass.getheapsizeperinstance() to use longs too. there isn't an immediate need, but it makes the parser code cleaner, and it's better to change the apis in one go. the api is now fixed for large arrays. methods of encoding a large size will need to be done later. (i snapshot ) (i snapshot ) (i snapshot ) (i object ) (abstract object impl ) (class impl ) (class impl ) (class impl ) (instance impl ) (object array ) (primitive array ) (class impl ) (class impl ) (array by size query ) (thread info impl ) (dominator query ) (garbage cleaner ) (snapshot impl ) (snapshot exception ) (size index collector uncompressed ) (int index collector uncompressed ) (size index reader ) (int index reader ) (int index streamer ) (size index reader ) (index manager ) (size index reader ) (class impl ) (class impl ) (i class ) (ap is ) 308310 308310 308310 308310